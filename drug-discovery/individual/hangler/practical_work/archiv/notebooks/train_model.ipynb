{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline training of an end-to-end clip\n",
    "# Encoder/decoder.\n",
    "# Try to return to torch_emb=False\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.datapipes.iter import FileLister, Shuffler\n",
    "\n",
    "from coati.data.dataset import COATI_dataset\n",
    "from coati.common.s3 import cache_read\n",
    "from coati.training.train_coati import train_autoencoder, do_args\n",
    "from coati.common.util import dir_or_file_exists, makedir, query_yes_no\n",
    "from coati.common.s3 import copy_bucket_dir_from_s3\n",
    "from coati.data.batch_pipe import UnstackPickles, UrBatcher, stack_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/Users/stefanhangler/Documents/Uni/Msc_AI/3_Semester/Seminar_Practical Work/Code.nosync/COATI/examples/practical_work_tests/\"\n",
    "DATA_PATH = \"datasets/chembl_smiles_coors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COATI Dataset Class (adapted)\n",
    "smiles_to_3d is a new function that takes a SMILES string and returns a 3D structure. It uses RDKit to generate the 3D structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COATI_dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir,\n",
    "        fields=[\"smiles\", \"atoms\", \"coords\"],\n",
    "        test_split_mode=\"row\",\n",
    "        test_frac=0.02,  # in percent.\n",
    "        valid_frac=0.02,  # in percent.\n",
    "    ):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.summary = {\"dataset_type\": \"coati\", \"fields\": fields}\n",
    "        self.test_frac = test_frac\n",
    "        self.fields = fields\n",
    "        self.valid_frac = valid_frac\n",
    "        assert int(test_frac * 100) >= 0 and int(test_frac * 100) <= 50\n",
    "        assert int(valid_frac * 100) >= 0 and int(valid_frac * 100) <= 50\n",
    "        assert int(valid_frac * 100 + test_frac * 100) < 50\n",
    "        self.test_split_mode = test_split_mode\n",
    "\n",
    "    def partition_routine(self, row):\n",
    "        \"\"\"Partitioning logic for dataset splits.\"\"\"\n",
    "        if not \"mod_molecule\" in row:\n",
    "            tore = [\"raw\"]\n",
    "            tore.append(\"train\")\n",
    "            return tore\n",
    "        else:\n",
    "            tore = [\"raw\"]\n",
    "\n",
    "            if row[\"mod_molecule\"] % 100 >= int(\n",
    "                (self.test_frac + self.valid_frac) * 100\n",
    "            ):\n",
    "                tore.append(\"train\")\n",
    "            elif row[\"mod_molecule\"] % 100 >= int((self.test_frac * 100)):\n",
    "                tore.append(\"valid\")\n",
    "            else:\n",
    "                tore.append(\"test\")\n",
    "\n",
    "            return tore\n",
    "\n",
    "    # new function to convert SMILES to 3D coordinates\n",
    "    def smiles_to_3d(self, row):\n",
    "        \"\"\"Convert SMILES string to 3D representation and optimize with MMFF94s.\"\"\"\n",
    "        smi = row['smiles']\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            mol = Chem.AddHs(mol)\n",
    "            AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "            AllChem.MMFFOptimizeMolecule(mol, mmffVariant='MMFF94s')\n",
    "            coords = mol.GetConformer().GetPositions()\n",
    "            row['coords'] = coords\n",
    "            row['atoms'] = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "        return row\n",
    "\n",
    "    def get_data_pipe(\n",
    "        self,\n",
    "        rebuild=False,\n",
    "        batch_size=32,\n",
    "        partition: str = \"raw\",\n",
    "        required_fields=[],\n",
    "        distributed_rankmod_total=None,\n",
    "        distributed_rankmod_rank=1,\n",
    "        xform_routine=None,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"Set up the data pipeline with RDKit processing included.\"\"\"\n",
    "        print(f\"trying to open a {partition} datapipe for...\")\n",
    "        if (\n",
    "            not dir_or_file_exists(os.path.join(self.cache_dir, DATA_PATH, \"0.pkl\"))\n",
    "        ) or rebuild:\n",
    "            makedir(self.cache_dir)\n",
    "            # Automatically proceed with downloading the data without asking for confirmation\n",
    "            copy_bucket_dir_from_s3(DATA_PATH, self.cache_dir)\n",
    "\n",
    "        # Use the smiles_to_3d function as the transformation routine\n",
    "        xform_routine = self.smiles_to_3d if xform_routine is None else xform_routine\n",
    "\n",
    "        pipe = (\n",
    "            FileLister(\n",
    "                root=os.path.join(self.cache_dir, DATA_PATH),\n",
    "                recursive=False,\n",
    "                masks=[\"*.pkl\"],\n",
    "            )\n",
    "            .shuffle()\n",
    "            .open_files(mode=\"rb\")\n",
    "            .unstack_pickles()\n",
    "            .unbatch()\n",
    "            .shuffle(buffer_size=200000)\n",
    "            .map(xform_routine)\n",
    "        )\n",
    "        pipe = pipe.ur_batcher(\n",
    "            batch_size=batch_size,\n",
    "            partition=partition,\n",
    "            xform_routine=xform_routine,\n",
    "            partition_routine=self.partition_routine,\n",
    "            distributed_rankmod_total=distributed_rankmod_total,\n",
    "            distributed_rankmod_rank=distributed_rankmod_rank,\n",
    "            direct_mode=False,\n",
    "            required_fields=self.fields,\n",
    "        )\n",
    "\n",
    "        return pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ChEMBL canonical smile strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Chembl dataset smile strings\n",
    "with cache_read(\"s3://terray-public/datasets/chembl_canonical_smiles.pkl\", \"rb\") as f:\n",
    "    chembl_canonical_smiles = pickle.loads(f.read(), encoding=\"UTF-8\")\n",
    "\n",
    "# Shuffle the dataset and select a subset for the example\n",
    "random.shuffle(chembl_canonical_smiles)\n",
    "chembl_subset = chembl_canonical_smiles[:10_000]\n",
    "chembl_subset = [{\"smiles\": s} for s in chembl_subset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'smiles': 'COc1cccc(NC(=O)CCC2CCN(C(=O)c3cncs3)CC2)c1'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chembl_subset[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in pickle file for faster read/write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a cache_dir variable defined as in the COATI_dataset class\n",
    "subset_file_path = os.path.join(cache_dir, DATA_PATH, \"chembl_subset.pkl\")\n",
    "with open(subset_file_path, \"wb\") as f:\n",
    "    pickle.dump(chembl_subset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST Datset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to open a raw datapipe for...\n",
      "Will download ~340 GB of data to /Users/stefanhangler/Documents/Uni/Msc_AI/3_Semester/Seminar_Practical Work/Code.nosync/COATI/examples/practical_work_tests . This will take a while. Are you sure? [y/n] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m COATI_dataset(\n\u001b[1;32m      3\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m      4\u001b[0m     fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmiles\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matoms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     valid_frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Call the get_data_pipe method to get the data pipeline\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m data_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrebuild\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True if you want to force a rebuild of the data cache\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The batch size for training\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Which partition of the data to use: 'raw', 'train', 'valid', or 'test'\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Other parameters can be left as defaults or specified according to your needs\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# You can then iterate over data_pipe to access your data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_pipe:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Process each batch as needed\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/molGen_master/lib/python3.9/site-packages/coati/data/dataset.py:77\u001b[0m, in \u001b[0;36mCOATI_dataset.get_data_pipe\u001b[0;34m(self, rebuild, batch_size, partition, required_fields, distributed_rankmod_total, distributed_rankmod_rank, xform_routine)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m dir_or_file_exists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir, S3_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     75\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m rebuild:\n\u001b[1;32m     76\u001b[0m     makedir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir)\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mquery_yes_no\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWill download ~340 GB of data to \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m . This will take a while. Are you sure?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     copy_bucket_dir_from_s3(S3_PATH, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir)\n\u001b[1;32m     82\u001b[0m pipe \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     83\u001b[0m     FileLister(\n\u001b[1;32m     84\u001b[0m         root\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir, S3_PATH),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;241m.\u001b[39mshuffle(buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200000\u001b[39m)\n\u001b[1;32m     93\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/molGen_master/lib/python3.9/site-packages/coati/common/util.py:205\u001b[0m, in \u001b[0;36mquery_yes_no\u001b[0;34m(question, default)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(question \u001b[38;5;241m+\u001b[39m prompt)\n\u001b[0;32m--> 205\u001b[0m     choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m valid[default]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/molGen_master/lib/python3.9/site-packages/ipykernel/kernelbase.py:1261\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/molGen_master/lib/python3.9/site-packages/ipykernel/kernelbase.py:1304\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Create an instance of the COATI_dataset class\n",
    "dataset = COATI_dataset(\n",
    "    cache_dir=cache_dir,\n",
    "    fields=[\"smiles\", \"atoms\", \"coords\"],\n",
    "    test_split_mode=\"row\",\n",
    "    test_frac=0.02,\n",
    "    valid_frac=0.02\n",
    ")\n",
    "\n",
    "# Call the get_data_pipe method to get the data pipeline\n",
    "data_pipe = dataset.get_data_pipe(\n",
    "    rebuild=False,  # Set to True if you want to force a rebuild of the data cache\n",
    "    batch_size=32,  # The batch size for training\n",
    "    partition=\"raw\",  # Which partition of the data to use: 'raw', 'train', 'valid', or 'test'\n",
    "    # Other parameters can be left as defaults or specified according to your needs\n",
    ")\n",
    "\n",
    "# You can then iterate over data_pipe to access your data\n",
    "for batch in data_pipe:\n",
    "    # Process each batch as needed\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molGen_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
